{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "ابتدا کتابخانه numpy را import می کنیم. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "سپس تابع های کمکی مورد نیاز را تعریف میکنیم. تابع سیگموئید و مشتق آن مورد نیاز است چرا که به عنوان تابع فعال\n",
    "ساز در MLP استفاده خواهیم کرد که forwad propagattion کاربرد دارد همچنین مشتق آن در backward propagation\n",
    "و انجام gradiant decent کاربرد دارد. همچین از تابع XOR برای تولید دیتا استفاده میکنیم. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the XNOR function\n",
    "def xnor(x1, x2):\n",
    "    return 1 if x1 == x2 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "حال داده های آموزشی را تعریف میکنیم. داده ها شامل دو ورودی و یک بایاس می باشند. خروجی\n",
    "هم با تابع xor که در بالا تعریف شد محاسبه شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training data\n",
    "# X=[x1, x2, bias]\n",
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "y = np.array([[xnor(x[0], x[1])] for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "حال هایپر پارامتر های مدل را تعریف میکنیم. شامل تعدا نورون های ورودی، مخفی و خروجی به همراه\n",
    "نرخ آموزش و تعداد مراحل آموزش می شوند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_layer_size = 3 # X = [x1, x2, bias]\n",
    "hidden_layer_size = 4\n",
    "output_layer_size = 1 # y = 0 or 1\n",
    "learning_rate = 0.1\n",
    "num_epochs = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "حال پارامتر های مدل را به شکل تصادفی مقداردهی میکنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights\n",
    "weights1 = np.random.uniform(size=(input_layer_size, hidden_layer_size))\n",
    "weights2 = np.random.uniform(size=(hidden_layer_size, output_layer_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در اینجا حلقه اصلی آموزش مدل را مشاهده میکنید. ابتدا forward pass را انجام میدهیم.\n",
    "سپس با استفاده از الگوریتم کاهش گرادیان backward pass را انجام می دهیم تا مدل آموزش ببیند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      " MSE: 0.3275488206466855\n",
      "------------------\n",
      "Epoch 1000:\n",
      " MSE: 0.24361481769312734\n",
      "------------------\n",
      "Epoch 2000:\n",
      " MSE: 0.20202365883301074\n",
      "------------------\n",
      "Epoch 3000:\n",
      " MSE: 0.11617936281879207\n",
      "------------------\n",
      "Epoch 4000:\n",
      " MSE: 0.03866641130675412\n",
      "------------------\n",
      "Epoch 5000:\n",
      " MSE: 0.015071734297531007\n",
      "------------------\n",
      "Epoch 6000:\n",
      " MSE: 0.008066388161991966\n",
      "------------------\n",
      "Epoch 7000:\n",
      " MSE: 0.0051987316717574845\n",
      "------------------\n",
      "Epoch 8000:\n",
      " MSE: 0.0037312930078802777\n",
      "------------------\n",
      "Epoch 9000:\n",
      " MSE: 0.002865833036821892\n",
      "------------------\n",
      "Epoch 10000:\n",
      " MSE: 0.0023044763487165823\n",
      "------------------\n",
      "Epoch 11000:\n",
      " MSE: 0.0019150079627639146\n",
      "------------------\n",
      "Epoch 12000:\n",
      " MSE: 0.0016309792663950031\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    hidden_layer_output = sigmoid(np.dot(X, weights1))\n",
    "    output_layer_output = sigmoid(np.dot(hidden_layer_output, weights2))\n",
    "\n",
    "    # Backward pass (computing gradiant with respect to loss)\n",
    "    output_layer_error = y - output_layer_output # compute loss\n",
    "    output_layer_delta = output_layer_error * sigmoid_derivative(output_layer_output)\n",
    "\n",
    "    hidden_layer_error = output_layer_delta.dot(weights2.T)\n",
    "    hidden_layer_delta = hidden_layer_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Update the weights using gradiant decent\n",
    "    weights2 += hidden_layer_output.T.dot(output_layer_delta) * learning_rate\n",
    "    weights1 += X.T.dot(hidden_layer_delta) * learning_rate\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}:\\n MSE: {np.mean(np.power(output_layer_error, 2))}\\n------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در اینجا تست می کنیم که خروجی مدل برای داده های تست که همان چهار حالت موجود چگونه است.\n",
    "همان طور که میبینیم در اینجا با پارامتر های یاد گرفته شده فقط forward propagation را انجام می دهیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [x1, x2, bias], Prediction: {0, 1}, model output\n",
      "Input: [0 0 1]       , Prediction: 1     , 0.9746917509729957\n",
      "Input: [0 1 1]       , Prediction: 0     , 0.03934005011560922\n",
      "Input: [1 0 1]       , Prediction: 0     , 0.04312803894173172\n",
      "Input: [1 1 1]       , Prediction: 1     , 0.9536002720660361\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# X=[x1, x2, bias]\n",
    "test_data = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "\n",
    "print(\"Input: [x1, x2, bias], Prediction: {0, 1}, model output\")\n",
    "for x in test_data:\n",
    "    prediction = sigmoid(np.dot(sigmoid(np.dot(x, weights1)), weights2))\n",
    "    print(f\"Input: {x}       , Prediction: {round(prediction[0])}     , {prediction[0]}\")\n",
    "    # print(f\"Input: {x}       , Prediction: {prediction[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
